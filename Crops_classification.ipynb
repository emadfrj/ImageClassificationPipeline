{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbeb03ef-5f95-4ba1-84cd-bb25c0b9b48a",
   "metadata": {},
   "source": [
    "# Crops Plants Image Classification\n",
    "In this example, we use the AutoImageClassification library to create a model to classify crops plants images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88dc614-f07f-4a65-ac6a-f6b3c3cd872c",
   "metadata": {},
   "source": [
    "### Importing neccessary moduls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4cc702-d736-49c9-bbee-792abe9f0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoImageClassification library moduls\n",
    "import AutoImageClassification.Bing_Image_Download as BID \n",
    "import AutoImageClassification.ImageEmbedder as IE \n",
    "import AutoImageClassification.Anomaly as AN \n",
    "import AutoImageClassification.ImageClassification as IC\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96db07-f26a-47ce-8a50-0dff0d3fce12",
   "metadata": {},
   "source": [
    "## Constant values\n",
    "Names of folders and files for storing models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4161222f-d079-435b-bf43-643a7e3e9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labeled_folder = \"Agricultural-crops\" # Path to the main folder of the labeled dataset\n",
    "Downloaded_Images_folder = 'downloaded_images' # Path to store the scraped images\n",
    "Max_num_downloads = 150 # Maximum number of scraped images per category\n",
    "\n",
    "Emb_path = 'Embeddings features' # Path to store all image embedding features\n",
    "Emb_downloaded = 'downloaded' # Subfolder name for the embedding features of the downloaded images\n",
    "Emb_labeled = 'labeled' # Subfolder name for the embedding features of the labeled images \n",
    "Emb_downloaded_clean = 'downloaded_clean' # Subfolder name for the embedding features of the downloaded images after filtering irrelevant images\n",
    "Emb_final_dataset = 'final_dataset' # Subfolder name for the embedding features of the final dataset containing train and test dataset\n",
    "\n",
    "Model_path = 'Models' # Path to store all the trained models\n",
    "Pseudo_Labeling_Model_Name ='Pseudo_Labeling_Model.pth' # Filename to store the classification model used for pseudo-labeling\n",
    "Final_Model_Name ='ImageClassification_Model.pth' # Filename to store the final classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32b689-464b-4c84-bee1-15b39ed661b4",
   "metadata": {},
   "source": [
    "## Image Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d34c76-a23a-41a7-be91-c95e07f3c61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using folder structure of labeled data, we list categories and search keywords \n",
    "Categories = [d for d in os.listdir(Labeled_folder) if os.path.isdir(os.path.join(Labeled_folder, d))]\n",
    "Keywords = [cat + ' plant' for cat in Categories]\n",
    "# Class to download bing images\n",
    "crops_image = BID.Bing_Image_Download(Max_num_downloads, Downloaded_Images_folder, Categories, Keywords)\n",
    "crops_image.Downloading() # Start downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688c9e6-da85-473b-ad5e-501734801bed",
   "metadata": {},
   "source": [
    "## Embedding \n",
    "Extract embedding features of downloaded and labeled images using pretrained EfficientNetB7 model and save it into .npz files. Create one file for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061e4ed-ba07-4a62-abec-386d5213c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageEmbedder = IE.ImageEmbedder(batch_size=4)\n",
    "\n",
    "Emb_path_downloaded = os.path.join(Emb_path, Emb_downloaded)\n",
    "ImageEmbedder.Embedding(Downloaded_Images_folder, Emb_path_downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7befcf6-e416-467a-b554-f6e851d76d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Emb_path_labled = os.path.join(Emb_path, Emb_labeled)\n",
    "ImageEmbedder.Embedding(Labeled_folder, Emb_path_labled, augment = True) # Perform image augmentation before creating embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb4808b-de6d-4fb2-9b3e-078b42d7633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding features and store them into a pandas dataframes \n",
    "Emb_path_downloaded = os.path.join(Emb_path, Emb_downloaded)\n",
    "Emb_path_labled = os.path.join(Emb_path, Emb_labeled)\n",
    "\n",
    "df_Downloaded = IE.ImageEmbedder.Loading_Embeddings(Emb_path_downloaded)\n",
    "df_Labeled = IE.ImageEmbedder.Loading_Embeddings(Emb_path_labled)\n",
    "\n",
    "df_all = pd.concat([df_Downloaded,df_Labeled], ignore_index=True) # using all the images (labeled and downloaded) in anomoly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e20e5-0648-4da7-b749-2a3eae480907",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "Finding Anomalies in downloaded images using autoencoder recunstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349b800-4d60-4655-bbf1-967c7f73bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(df_all['Embedding'])  # shape (N, D) suitable for training\n",
    "input_dimention = len(X[0]) # embedding size\n",
    "model_anomaly = AN.Anomaly(input_dimention) \n",
    "model_anomaly.train_autoencoder(X,model_output_path = Model_path,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b67d2a3-d481-4b01-ab21-b585ba2c794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly and reconstruction error to downloaded dataframe\n",
    "X_Downloaded = np.vstack(df_Downloaded['Embedding'])  # shape (N, D)\n",
    "anomaly_df = model_anomaly.anomaly_detecting(X_Downloaded)\n",
    "df_Downloaded['Anomaly'] = anomaly_df[0] # 0 for normals, 1 for anomalies\n",
    "df_Downloaded['Anomaly_error'] = anomaly_df[1]\n",
    "# Normalized anomaly error\n",
    "min_val = df_Downloaded['Anomaly_error'].min()\n",
    "max_val = df_Downloaded['Anomaly_error'].max()\n",
    "range_val = max_val - min_val\n",
    "df_Downloaded['Anomaly_error_normalized'] = (df_Downloaded['Anomaly_error'] - min_val) / range_val "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9056959-fd78-422e-b399-0ec18f1a262d",
   "metadata": {},
   "source": [
    "## Pseudo Labeling\n",
    "Train a model using all labeled data (train and test) to Pseudo label the downloaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ecc95-227f-4a3a-984d-44c9b963f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pseudo_Labeling_model = IC.ImageClassification()\n",
    "\n",
    "# Define hyperparameter grid for tuning the model\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 3e-4],\n",
    "    'dropout': [0.3, 0.5],\n",
    "    'hidden_size': [128,256]\n",
    "}\n",
    "# Train the model and save it into a .pth file \n",
    "Pseudo_Labeling_model.Training_Tuning(param_grid, df_Labeled, model_output_path = Model_path, model_output_name = Pseudo_Labeling_Model_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c98ee68c-ebe7-46d5-8ead-1f8045b17a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "Pseudo_model_path = os.path.join(Model_path, Pseudo_Labeling_Model_Name)\n",
    "Pseudo_Labeling_model_loaded = IC.ImageClassification()\n",
    "Pseudo_Labeling_model_loaded.load_model(Pseudo_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24f9c1b8-8e8f-488f-86ed-12cd94cf32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign predicted class and the probability of the predicted class to df_Downloaded using the pseudo-labeling model\n",
    "df_Downloaded['predictions'] = Pseudo_Labeling_model_loaded.predict_probabilities(df_Downloaded)\n",
    "df_Downloaded['predicted_class'] = df_Downloaded['predictions'].apply(lambda x: x.index(max(x)))\n",
    "df_Downloaded['predicted_class_probability'] = df_Downloaded['predictions'].apply(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd9a17-42b2-4d10-bea9-2d96a1b8cc13",
   "metadata": {},
   "source": [
    "# Clean downloaded images\n",
    "Combining predicted_class_probability and Anomaly_error_normalized to estimate a confidence score for downloaded images. \n",
    "If the predicted label matches the original category (search keyword) and the confidence is high enough, keep the image. Otherwise, discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52a8594a-4013-4643-91a0-851d4056f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Downloaded['confidence_score'] = df_Downloaded['predicted_class_probability'] * (1 -   0.4 * df_Downloaded['Anomaly_error_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973dd14-eb30-434a-961c-07df0dada3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence score distribution to choose a suitable cut-off.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confidence_distribution(df, column='confidence_score', bins=50):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df[column], bins=bins, kde=True, color='skyblue')\n",
    "    plt.title('Distribution of Confidence Score')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confidence_distribution(df_Downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141bfc5-a4c9-43f8-b3c1-9aeab85cc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retain images that are correctly labeled and/or have a confidence score > 0.8.\n",
    "# Using 'or' makes the condition less strict.\n",
    "df_Downloaded_Clean = df_Downloaded[\n",
    "    (df_Downloaded['predicted_class'] == df_Downloaded['Cat_Index']) |\n",
    "    (df_Downloaded['confidence_score'] > 0.7)\n",
    "]\n",
    "#Drop the unnecessary columns\n",
    "df_Downloaded_Clean = df_Downloaded_Clean.iloc[:, :3]\n",
    "\n",
    "discarded_images_percent = 100 * ((df_Downloaded.shape)[0] - (df_Downloaded_Clean.shape)[0] ) / (df_Downloaded.shape)[0]\n",
    "print(f'The percentage of images that are discarded:{discarded_images_percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "421c4d65-3c2a-40aa-981e-d9a859517349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeding of the cleaned downloaded images\n",
    "CleanDownloaded_filename = f\"CleanDownloadedEmbedding.npz\"\n",
    "Emb_path_downloaded_clean = os.path.join(Emb_path, Emb_downloaded_clean)\n",
    "os.makedirs(Emb_path_downloaded_clean, exist_ok=True)\n",
    "embedding_file = os.path.join(Emb_path_downloaded_clean, CleanDownloaded_filename)\n",
    "np.savez(embedding_file, embeddings=df_Downloaded_Clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4df95d-94f9-4506-8fc4-c68f5c4de166",
   "metadata": {},
   "source": [
    "## Image Classification Model\n",
    "Using both labeled dataset and cleaned downloaded images to train the image classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0465a1f0-e737-4390-82a3-8ff75065b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Downloaded_Clean dataset\n",
    "Emb_path_Downloaded_Clean= os.path.join(Emb_path,Emb_downloaded_clean, CleanDownloaded_filename)\n",
    "df_Downloaded_Clean = IE.ImageEmbedder.Load_one_embedding(Emb_path_Downloaded_Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13747419-50eb-4ea7-8b85-c17b4604b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset\n",
    "df_final = pd.concat([df_Downloaded_Clean,df_Labeled], ignore_index=True)\n",
    "# create train/test dataset and save it into two files \n",
    "train_df, test_df = train_test_split(df_final, test_size = 0.2, stratify=df_final['Cat_Index'], random_state=42)\n",
    "Emb_path_finaldataset = os.path.join(Emb_path, Emb_final_dataset)\n",
    "os.makedirs(Emb_path_finaldataset, exist_ok=True)\n",
    "train_file = os.path.join(Emb_path_finaldataset, f\"Train.npz\")\n",
    "test_file = os.path.join(Emb_path_finaldataset, f\"Test.npz\")\n",
    "np.savez(train_file, embeddings=train_df)\n",
    "np.savez(test_file, embeddings=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6026a-ee87-4823-aeb0-25acf37f95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "ImageClassification_model = IC.ImageClassification()\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 3e-4],\n",
    "    'dropout': [0.3, 0.5],\n",
    "    'hidden_size': [128,256]\n",
    "}\n",
    "ImageClassification_model.Training_Tuning(param_grid, train_df, model_output_path = Model_path, model_output_name = Final_Model_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca6ff1-ccab-44e4-addc-1ae2fa50e67e",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60420ed1-4be5-46c1-97c5-baf7654db01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "Final_Model_path = os.path.join(Model_path,Final_Model_Name)\n",
    "Model_loaded = IC.ImageClassification()\n",
    "Model_loaded.load_model(Final_Model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fcd0134-db95-4d38-82c6-8bbce052d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "Emb_path_test= os.path.join(Emb_path,Emb_final_dataset,'Test.npz')\n",
    "df_test = IE.ImageEmbedder.Load_one_embedding(Emb_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b78281-d0a3-43b5-b20a-1e892b373421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using test dataset\n",
    "Model_loaded.validate(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
